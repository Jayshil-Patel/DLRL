{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_data(data_folder):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_path = os.path.join(data_folder, label)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                review = file.read()\n",
    "                reviews.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    \n",
    "    return reviews, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data_folder_train = 'D:\\\\jaysh\\\\FALL2023\\\\DLRL\\\\HW3\\\\dataset\\\\train'  # Replace with the actual path to your dataset\n",
    "data_folder_test = 'D:\\\\jaysh\\\\FALL2023\\\\DLRL\\\\HW3\\\\dataset\\\\test'  # Replace with the actual path to your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data(data_folder_train)\n",
    "X_test, y_test = load_data(data_folder_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train_tfidf.toarray()\n",
    "X_test = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model20(hp):\n",
    "    vocab_size = 5000\n",
    "    embedding_size = 128\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=20))\n",
    "    model.add(SimpleRNN(units=128, input_shape=(None, 1), dropout=hp.Choice(\"dropout\", values=[0.0, 0.4, 0.5, 0.6])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "def build_model50(hp):\n",
    "    vocab_size = 5000\n",
    "    embedding_size = 128\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=50))\n",
    "    model.add(SimpleRNN(units=128, input_shape=(None, 1), dropout = hp.Choice(\"dropout\", values=[0.0, 0.4, 0.5, 0.6])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #print(model.summary())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                 optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])), \n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model100(hp):\n",
    "    vocab_size = 5000\n",
    "    embedding_size = 128\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=100))\n",
    "    model.add(SimpleRNN(units=128, input_shape=(None, 1), dropout = hp.Choice(\"dropout\", values=[0.0, 0.4, 0.5, 0.6])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #print(model.summary())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                 optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])), \n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model200(hp):\n",
    "    vocab_size = 5000\n",
    "    embedding_size = 128\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=200))\n",
    "    model.add(SimpleRNN(units=128, input_shape=(None, 1), dropout = hp.Choice(\"dropout\", values=[0.0, 0.4, 0.5, 0.6])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #print(model.summary())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                 optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])), \n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model500(hp):\n",
    "    vocab_size = 5000\n",
    "    embedding_size = 128\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=500))\n",
    "    model.add(SimpleRNN(units=128, input_shape=(None, 1), dropout = hp.Choice(\"dropout\", values=[0.0, 0.4, 0.5, 0.6])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #print(model.summary())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                 optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])), \n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 59s]\n",
      "val_accuracy: 1.0\n",
      "\n",
      "Best val_accuracy So Far: 1.0\n",
      "Total elapsed time: 00h 02m 44s\n",
      "Results summary\n",
      "Results in my_dir\\helloworld\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.4\n",
      "learning_rate: 0.001\n",
      "Score: 1.0\n",
      "\n",
      "Trial 2 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.6\n",
      "learning_rate: 0.001\n",
      "Score: 1.0\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.6\n",
      "learning_rate: 0.01\n",
      "Score: 0.5\n",
      "\n",
      "********************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#20 state dimensions batch_size 32\n",
    "X20_train = sequence.pad_sequences(X_train, maxlen=20)\n",
    "X20_test = sequence.pad_sequences(X_test, maxlen=20)\n",
    "bsize = 32\n",
    "X_valid, y_valid = X20_train[:bsize], y_train[:bsize]\n",
    "X_train2, y_train2 = X20_train[bsize:], y_train[bsize:]\n",
    "tuner = RandomSearch(\n",
    "    build_model20,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "tuner.search(X_train2, y_train2, epochs=3, validation_data=(X_valid, y_valid), batch_size = bsize)\n",
    "tuner.results_summary()\n",
    "print(\"\\n********************************************************************************\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Assuming you have X_train, X_test, y_train, y_test\n",
    "\n",
    "# # Set hyperparameters\n",
    "# # state_dimensions = [20, 50, 100, 200, 500]\n",
    "# state_dimensions = [20]\n",
    "# embedding_dim = 100\n",
    "# max_len = 5000\n",
    "\n",
    "# # Tokenize the text\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Pad sequences to a fixed length\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# # Model building function\n",
    "# def build_rnn_model(state_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, trainable=True))\n",
    "#     model.add(SimpleRNN(units=state_dim))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def build_lstm_model(state_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, trainable=True))\n",
    "#     model.add(LSTM(units=state_dim))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Train and evaluate models\n",
    "# results_rnn = []\n",
    "# results_lstm = []\n",
    "\n",
    "# for state_dim in state_dimensions:\n",
    "#     # RNN\n",
    "#     rnn_model = build_rnn_model(state_dim)\n",
    "#     rnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "#     rnn_result = rnn_model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "#     results_rnn.append((state_dim, rnn_result[1]))  # Accuracy is at index 1\n",
    "    \n",
    "#     # LSTM\n",
    "#     lstm_model = build_lstm_model(state_dim)\n",
    "#     lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=64,  verbose=0)\n",
    "#     lstm_result = lstm_model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "#     results_lstm.append((state_dim, lstm_result[1]))\n",
    "\n",
    "# # Display results\n",
    "# print(\"RNN Results:\")\n",
    "# print(\"State Dimension\\tAccuracy\")\n",
    "# for result in results_rnn:\n",
    "#     print(f\"{result[0]}\\t\\t\\t{result[1]}\")\n",
    "\n",
    "# print(\"\\nLSTM Results:\")\n",
    "# print(\"State Dimension\\tAccuracy\")\n",
    "# for result in results_lstm:\n",
    "#     print(f\"{result[0]}\\t\\t\\t{result[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
